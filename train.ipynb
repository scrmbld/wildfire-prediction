{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "a52f1168",
            "metadata": {},
            "source": "# Training the model defined in lstm_model.py"
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "9eab292b",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "import os\nimport json\n\nimport numpy as np\nimport pandas as pd\n\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import TensorDataset, DataLoader\nfrom torch.optim.lr_scheduler import StepLR\n\nimport preproc"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f6f1aba7",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "N_EPOCHS = 200\nBATCH_SIZE = 64"
        },
        {
            "cell_type": "markdown",
            "id": "e83f438d",
            "metadata": {},
            "source": "## Load the dataset"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "55c2e4f3",
            "metadata": {
                "trusted": true
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": "['latitude', 'longitude', 'timestamp']"
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "data_dir = 'dataset/reduced_preprocessed'\ntrain = np.load(os.path.join(data_dir, 'train.npy'))\nvalid = np.load(os.path.join(data_dir, 'valid.npy'))\ntest = np.load(os.path.join(data_dir, 'test.npy'))\n\nbounds = {}\nbounds_path = os.path.join(data_dir, 'bounds.json')\nwith open(bounds_path) as file:\n    bounds = json.load(file)\n\n# the names of the columns in our dataset\ncols = list(bounds.keys())\ncols"
        },
        {
            "cell_type": "markdown",
            "id": "bf4c06e4",
            "metadata": {},
            "source": "## Initialize the model"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3827efc6",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # use a GPU if we can\n\n# These are some of the dimensions of the model\nevent_vec_dim = train.shape[-1] # the length of each vector in the sequence\nseq_len = preproc.WINDOW_SIZE # the length of the sequence\nhidden_dim = 1024 # number of neurons in the LSTM layers\nnum_layers = 4 # number of LSTM layers"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "74083f23",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "from lstm_model import FirePredictor\n\n# actually create the model\nmodel = FirePredictor(event_vec_dim, seq_len, hidden_dim, num_layers)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0bee3402",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# visualize model structure\n# https://github.com/mert-kurttutan/torchview\n\nimport graphviz\ngraphviz.set_jupyter_format('png')\n\nfrom torchview import draw_graph\nmodel_graph = draw_graph(model, input_size=(BATCH_SIZE, seq_len, event_vec_dim), device='cuda')\nmodel_graph.visual_graph"
        },
        {
            "cell_type": "markdown",
            "id": "21444a7b",
            "metadata": {},
            "source": "## Write the loss function\nNOTE: I ended up discarding these (because they kept spewing out NaNs everywhere). There are ways to deal with some of the variance in the dataset using the loss function so they may be worth revisiting later."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "50f31bf3",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# def euclidean_dist(t1, t2, dim):\n#     return (t1 - t2).pow(2).sum(dim).sqrt()\n#\n# def avg_distance(x, y):\n#     if x.isnan().any():\n#         print(x)\n#         raise RuntimeError(\"Loss func: x contains NAN\")\n#     # use nearest neighbor upsampling\n#     reshaped = torch.nn.functional.interpolate(x[:,:,None,:], size=y.shape[2:], mode='nearest')\n#     if reshaped.isnan().any():\n#         print(reshaped)\n#         raise RuntimeError(\"Loss func: reshaped x contains NAN\")\n#     result = (euclidean_dist(reshaped, y, dim=2)).sum() / np.array(y.shape[0:-1]).prod()\n#     if result.isnan().any():\n#         print(result)\n#         raise RuntimeError(\"Loss func: result contains NAN\")\n#     return result\n#\n#\n# x = torch.Tensor(sequences[0:3,0:64,0,:])\n# y = torch.Tensor(sequences[0:3,0:64,1:,:])\n#\n# print(avg_distance(torch.Tensor(x), torch.Tensor(y)))\n# print(f'{x.shape=}, {y.shape=}')"
        },
        {
            "cell_type": "markdown",
            "id": "3520f169",
            "metadata": {},
            "source": "## Prepare Optimizer, Scheduler, and Data Loader"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f748e488",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# split train & validation data into inputs and labels\n# too large to fit all of this on the GPU, so only move it when we load each batch\nx_train = torch.Tensor(train[:,:preproc.WINDOW_SIZE,:])\ny_train = torch.Tensor(train[:,preproc.WINDOW_SIZE,:])\n\n# this can all fit in GPU memory, so put it there now\nx_valid = torch.Tensor(valid[:,:preproc.WINDOW_SIZE,:]).to(device)\ny_valid = torch.Tensor(valid[:,preproc.WINDOW_SIZE,:]).to(device)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9368a617",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "print(f'{x_valid.shape=}, {y_valid.shape}')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "483ebc9f",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "optimizer = torch.optim.SGD(model.parameters(), lr=0.03, momentum=0.9)\n\nscheduler = StepLR(optimizer, step_size=10, gamma=0.5)\n\n# since we already put the data into sequences, we can shuffle it without breaking things\ndata_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=BATCH_SIZE, shuffle=True)\n\n# oh yeah, we use mean squared error\nloss_function = nn.MSELoss()"
        },
        {
            "cell_type": "markdown",
            "id": "2ae234c5",
            "metadata": {},
            "source": "## Training Loop\nThis is all very standard."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "bfe6b02a",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# we store some performance metrics in these\nlosses_valid = np.zeros(N_EPOCHS)\nlosses_train = np.zeros(N_EPOCHS)\n\nfor epoch in range(N_EPOCHS):\n    # loop over batches\n\n    # add up the loss across all batches\n    acc_train_loss = 0.0\n    for i, data in enumerate(data_loader):\n\n        x_batch, y_batch = data\n        # send our batch over to the GPU (if we have one)\n        x_batch = x_batch.to(device)\n        y_batch = y_batch.to(device)\n\n        optimizer.zero_grad()\n\n        preds = model(x_batch)\n\n        loss = loss_function(preds, y_batch)\n        acc_train_loss += loss # save the loss\n        loss.backward()\n\n        optimizer.step()\n\n    # record average loss of all batches in this epoch\n    losses_train[epoch] = acc_train_loss / (x_train.shape[0]/BATCH_SIZE)\n\n    # compute validation loss\n    model.eval() # disable training stuff (not exactly sure what this does)\n    with torch.no_grad(): # disable gradient calculation\n        v_preds = model(x_valid)\n        losses_valid[epoch] = loss_function(v_preds, y_valid).item()\n    model.train()\n\n    # print out the losses so we can see it update as we train\n    print(f'Epoch {epoch} -- train loss: {losses_train[epoch]} valid loss: {losses_valid[epoch]}')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "c04cd332",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# print out a random prediction to prove it works\nmodel(x_train[0][None, :, :].to(device))"
        },
        {
            "cell_type": "markdown",
            "id": "c0f62097",
            "metadata": {},
            "source": "# Analyze the results"
        },
        {
            "cell_type": "markdown",
            "id": "f9308854",
            "metadata": {},
            "source": "## Plot the losses"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9b71bfae",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "import matplotlib.pyplot as plt\nplt.plot(losses_train, label='train loss')\nplt.plot(losses_valid, label='validation loss', color='r')\nplt.legend()\n\nplt.show()"
        },
        {
            "cell_type": "markdown",
            "id": "583d258a",
            "metadata": {},
            "source": "## Compare predictions against ground truth real quick\nSee eval_model.ipynb for more detailed analysis."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5028e31c",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# calculate some predictions\nmodel.eval()\nwith torch.no_grad():\n    res = model(x_train[0:128].to(device)).cpu().detach()\n    res = pd.DataFrame(res, columns=cols)\n\n# undo preprocessing\nprediction = preproc.unprocess(res, bounds)\nactual = preproc.unprocess(pd.DataFrame(y_train[0:128].numpy(), columns=cols), bounds)\n\n# display the dataframes\ndisplay(prediction)\ndisplay(actual)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4e135f80",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# sort them both by timestamp to make them easier to compare\ndisplay(prediction.sort_values('timestamp'))\ndisplay(actual.sort_values('timestamp'))"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "9e0dc10f",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# compute standard deviation to tell us if the model is covering about the right size of range\ndisplay(prediction.std(axis=0))\ndisplay(actual.std(axis=0))"
        },
        {
            "cell_type": "markdown",
            "id": "f9ecbfb5",
            "metadata": {},
            "source": "# Save the model's state_dict to a JSON file to evaluate later\nWARNING: These are 600+MiB on a single line. Be careful when opening them!"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "3c12cfcb",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "model_path = \"models/reduced_lstm.json\"\nmodel.to_json(model_path)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "df804c5f",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# load the model, just to be sure that we can\nmodel.from_json(model_path)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "add8afc1",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# use the model to make sure that the loaded one is the same as the original one\nmodel.eval()\nwith torch.no_grad():\n    res = model(x_train[0:128].to(device)).cpu().detach()\n    res = pd.DataFrame(res, columns=cols)\n\npred2 = preproc.unprocess(res, bounds)"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a8e3b47b",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# do the same sorting stuff as before\ndisplay(prediction.sort_values('timestamp'))\ndisplay(pred2.sort_values('timestamp'))"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}