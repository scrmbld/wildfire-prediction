{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "79d2aa2e",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "import json\nimport pandas as pd\nimport numpy as np\nimport os\n\nfrom sklearn.model_selection import train_test_split\n\nimport preproc"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a370e64b",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# the percentage of the dataset that will be used for validation and testing data\nVALID_SIZE = 0.2\nTEST_SIZE = 0.2"
        },
        {
            "cell_type": "markdown",
            "id": "642ecf64",
            "metadata": {},
            "source": "## Load the original data\nWe also do our feature selection here. We only use latitude, longitude, and timestamp."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "4b5a937d",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "infile_path = 'dataset/modis_reduced/fire_archive_M6_96619.csv'\ndataset_f = pd.read_csv(infile_path)\ndataset_f.drop(['bright_t31', 'daynight', 'confidence', 'frp', 'brightness'], axis=1, inplace=True)"
        },
        {
            "cell_type": "markdown",
            "id": "b690237b",
            "metadata": {},
            "source": "## Preprocess the dataset using funtions from preproc.py"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "8e9ec71f",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# get the minimum & maximum values of each column\nbounds = preproc.get_bounds(dataset_f)\n\n# this scales all of our data\n# this is also where I would turn tokens into integers... IF I HAD ANY\ndataset_f = preproc.preprocess(dataset_f, bounds)\ndataset_f.head()"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "03f5688a",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "bounds"
        },
        {
            "cell_type": "markdown",
            "id": "f971383f",
            "metadata": {},
            "source": "## Arrange the dataset into sequences"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5e6710bb",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "# split into x and y values\ndset_x, dset_y = preproc.xy_split(dataset_f)\n# use a sliding window algorithm to make the sequences\nsequences = preproc.sequencify(dset_x, dset_y)"
        },
        {
            "cell_type": "markdown",
            "id": "4ff4bc09",
            "metadata": {},
            "source": "## Split into training, validation, and testing"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "f5319afc",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "train, t2 = train_test_split(sequences, shuffle=False, test_size=(TEST_SIZE+VALID_SIZE))\nvalid, test = train_test_split(t2, shuffle=False, test_size =TEST_SIZE / (TEST_SIZE+VALID_SIZE))\n\nprint(f'{train.shape=}, {valid.shape=}, {test.shape=}')"
        },
        {
            "cell_type": "markdown",
            "id": "e0047ed3",
            "metadata": {},
            "source": "## Write the preprocessed data to files\nUse numpy for the train, test, and validation data (because it has too many dimensions for a csv), and JSON for the bounds because it's already a dict."
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5464a471",
            "metadata": {
                "trusted": true
            },
            "outputs": [],
            "source": "root_dir = 'dataset/reduced_preprocessed'\nos.makedirs(root_dir, exist_ok=True)\n\n# training data\ntrain_path = os.path.join(root_dir, 'train')\nnp.save(train_path, train)\n\n# validation data\nvalid_path = os.path.join(root_dir, 'valid')\nnp.save(valid_path, valid)\n\n# test data\ntest_path = os.path.join(root_dir, 'test')\nnp.save(test_path, test)\n\n# bounds\n# we need this to undo our preprocessing later (so that we can understand & graph the model outputs)\nbounds_path = os.path.join(root_dir, 'bounds.json')\nwith open(bounds_path, mode='w') as file:\n    json.dump(bounds, file)"
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3 (ipykernel)",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.12"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}